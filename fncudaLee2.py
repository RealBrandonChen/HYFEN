'''
Georgia Tech
@Author Peng CHEN
Created on 04-05-2024
Generate Lee Hologram with NVIDIA CUDA.
Environment: @CUDA 12.4 Toolkit @GPU RTX 3090
'''

import ctypes
import numpy as np
from cuda import cuda
from cuda_py_examples.common import common
from cuda_py_examples.common.helper_cuda import checkCudaErrors, findCudaDeviceDRV
import time

def cudaLee2(input_phases, hadamardSize, totalPoints, Lxy, f_carrier, rot):
    
    # CUDA Kernel
    cudaLee = """\

    /*
    This kernel takes the phases as the input and returns the binary Lee Holograms
    * out: returned uint8 CGH generated by Lee Holograms, stored in device
    * phases: float array of flattened phase pattern, stored in device
    * patterSize: int number of the size of phase pattern
    * Lxy: the magnification factor along both y and x direction of phase pattern
    * ref: boolean number to indicate if internal reference is required
    * Py: output pattern size in y direction
    * Px: output pattern size in x direction
    */
    #define CUDART_PI_F 3.14159265358979323846
    #define numColumnsPerFrame 128
    #define numRowsPerFrame  768
    #define stride 128
    #define frameSizeInBytes 128 * 768
    
    //Device code
    extern "C" __global__
    void LeeKernel(unsigned char *out, float *phases, int numReferencePixels, int leeBlockSize, int patternSizeX, int patternSizeY, float carrierFreq, float rot)
    {
        size_t column_global = blockIdx.x*blockDim.x + threadIdx.x;
        size_t y = blockIdx.y*blockDim.y + threadIdx.y;
        size_t x = column_global % numColumnsPerFrame;
        size_t z_local = column_global / numColumnsPerFrame;

        float alpha[8] = { 0, 0, 0, 0, 0, 0, 0, 0 };
        unsigned char B[8];

        if ((y >= numReferencePixels) && (y < numRowsPerFrame - numReferencePixels) && (x * 8 >= numReferencePixels) && (x * 8 < 768 - numReferencePixels))
        {
            // query inputs!
            int sampleY = (y - numReferencePixels) / leeBlockSize;
            for (int k = 0; k<8; k++)
            {
                int sampleX = (8 * x - numReferencePixels + k) / leeBlockSize;
                alpha[k] = phases[z_local*patternSizeY*patternSizeX + sampleY*patternSizeX + sampleX];
            }
        }

        for (int k = 0; k<8; k++)
        {
            float carrierWave = cos(rot)*(x * 8 + k) + sin(rot)*y; // carrier wave rotation
            B[k] = (0.5 * (1 + cos(2.0f * (float)CUDART_PI_F*(carrierWave)* (carrierFreq)-alpha[k]))) < 0.5;
        }

	
	    out[frameSizeInBytes*z_local + y * stride + x] = B[0] * 128 | B[1] * 64 | B[2] * 32 | B[3] * 16 | B[4] * 8 | B[5] * 4 | B[6] * 2 | B[7] * 1;
    }
    """
    startTime = time.time()
    # Initialize CUDA device
    checkCudaErrors(cuda.cuInit(0))
    cuDevice = findCudaDeviceDRV()
    # Create context
    cuContext = checkCudaErrors(cuda.cuCtxCreate(0, cuDevice))

    uvaSupported = checkCudaErrors(cuda.cuDeviceGetAttribute(cuda.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_UNIFIED_ADDRESSING, cuDevice))
    if not uvaSupported:
        print("Accessing pageable memory directly requires UVA")

    kernelHelper = common.KernelHelper(cudaLee, int(cuDevice))
    _Lee_kernel = kernelHelper.getFunction(b'LeeKernel')

    numPlanes = totalPoints
    NUM_THREADS_X = 32 # Threads per block, maximum NUM_THREADS_x*NUM_THREADS_y*NUM_THREADS_z = 1024
    NUM_THREADS_Y = 32
    NUM_BLOCKS_X = numPlanes*128 / NUM_THREADS_X
    NUM_BLOCKS_Y = 768 / NUM_THREADS_Y
    
    # Load parameters   
    patternSize = int(hadamardSize)
    leeBlockSize = int(Lxy)
    numReferencePixels = int((768 - patternSize * leeBlockSize)/2)
    carrierFreq = f_carrier
    rot = rot
    E_in = input_phases

    # Initialize buffer size for input phases and output patterns
    n_out = int(numPlanes * 128 * 768)
    n_phases = patternSize * patternSize * numPlanes
    bufferSize_phases = n_phases * (np.dtype(np.float32).itemsize)
    bufferSize_out = n_out * (np.dtype(np.uint8).itemsize)
    
    # Ravel numpy array to 1D array for C array generation (C pointers)
    E_in_ravel = E_in.transpose(1, 0, 2).ravel(order='F')
    
    h_phases = E_in_ravel.astype(dtype=np.float32)
    h_out = np.zeros(n_out).astype(dtype=np.uint8)

    # Allocate vectors in device memory
    d_phases = checkCudaErrors(cuda.cuMemAlloc(bufferSize_phases))
    d_out = checkCudaErrors(cuda.cuMemAlloc(bufferSize_out))

    checkCudaErrors(cuda.cuMemcpyHtoD(d_phases, h_phases, bufferSize_phases))
    
    # Convert parameters as C types (align with types in the kernel)
    kernelArgs = ((d_out, d_phases, numReferencePixels, leeBlockSize, patternSize, patternSize, carrierFreq, rot),
                (None, None, ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_float, ctypes.c_float))
    
    print("Starting kernel...")
    
    checkCudaErrors(cuda.cuLaunchKernel(
    _Lee_kernel,
    NUM_BLOCKS_X,  # grid x dim
    NUM_BLOCKS_Y,  # grid y dim
    1,  # grid z dim
    NUM_THREADS_X,  # block x dim
    NUM_THREADS_Y,  # block y dim
    1,  # block z dim
    0,  # dynamic shared memory
    0,  # stream
    kernelArgs,  # kernel arguments
    0,  # extra (ignore)
    ))
    
    endTime = time.time()
    print("Generate {0} patterns with {1} seconds.".format(numPlanes, np.round(endTime-startTime, 3)))
    # Copy result from device memory to host memory
    # h_C contains the result in host memory
    checkCudaErrors(cuda.cuMemcpyDtoH(h_out, d_out, bufferSize_out))
    cudaSeq = np.reshape(np.array(h_out), (totalPoints, 768,128)).transpose(1,2,0)

    # Free device memory
    checkCudaErrors(cuda.cuMemFree(d_phases))
    checkCudaErrors(cuda.cuMemFree(d_out))
    
    return cudaSeq